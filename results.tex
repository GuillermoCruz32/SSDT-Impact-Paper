\section{RESULTS}
To prove that the internship model works, it needed to be executed by the institution. The following will detail the success of the model at the institution after its implementation by describing how the software success was measured and interpret the data retrieved from the Product Owners.
This internship model has produced such favorable outcomes that it has been implemented consecutively for five years and  softwares are currently in use at the institution. During earlier implementations of the model, the software development team was very* ambitious and used this model on multiple softwares per year, hence there being more than five applications over the course of five years. However, at the beginning of the third iteration of the internship model, the software development team determined that the model can executed be more efficiently and effectively if only one system is being worked on per year.

\subsection{Measuring Success}
In order to demonstrate the impact of software developed predominantly by undergraduate students has on higher educational institutions, a success criteria had to be defined. To determine the impact that the student-developed software has had on the institution several factors of software success are identified and evaluated.  There exists much research on quantifying project success. The metrics for software success were defined using DeLone and McLean’s Model for Information Software Success. (CITATION NEEDED) This model consists of six measures of success which included the following: system quality, information quality, use, user satisfaction, individual impact, and organizational impact. These measures may be used to evaluate the software systems based on factors including but not limited to, code quality, appearance, usability, relevance, and cost reduction. The software developed by the SSDT uses ideas outlined in the Agile Manifesto. A survey comprised of questions that derive from each of these six components as well as principles from the Manifesto was sent out to Product Owners of software that was built by the software development team.

DeLone's and McLean's Model for Software Success were used to model the questions within the survey, particularly, the last four measures were focused on for the survey as the first two (system quality and information quality) are two factors that the Product Owners will not have much interaction with and is more technical, so they were not included in the survey. These four factors* are user satisfaction, use, individual impact, and organizational impact.

\subsection{Participants}
Given the small size of our nonprofit institution, a limited number of staff have administrative access. Participants in this study included 14 institutional faculty and staff. All participants in this study were volunteers and were administered a survey pertaining only to the software that they had interacted with. The survey was only sent to those who were the primary users and/or had administrative privileges to the software. This pool of faculty members was selected, as opposed to surveying all users, because these subjects were involved in the initial phase of the software development process, which include defining the software project’s scope and the needs of the customer. [CITATION?] Each survey began with an informed consent disclaimer that detailed the nature of the survey.

The SSDT has eight systems that are currently in production. `` These include the Advancement Office, BCAC, BCSR, CAS, LSF, Ulmann, URCPP, and SKYZ.'' [Take out?] Each system has a specific functionality to aid the faculty and staff clientele perform their jobs and the surveys were tailored to each Product Owner so that the questions only reflected applications that they have interacted with. For example, CAS (Course Administration and Scheduling) helps the user schedule terms, manage courses, programs, divisions and therefore the survey only had questions that pertained to these tasks.

The surveys were distributed and the Product Owners were given two weeks to submit their responses. The survey illicited just over 70 percent participation with ten out of fourteen clients answering. From these responses, data was collected on six out of the eight applications developed and managed by the student development team.

\subsection{Evaluating the Software}


\subsubsection{User satisfaction}
User satisfaction takes into consideration the ease of usefulness of the software. The questions in the survey asked about the simplicity of completing a specific task pertaining to the  software that Product Owner would have interacted with as well as how learnable the software is; this question asked them to rate this feature on a scale of one to four (four being very easy to use and very learnable, one being very difficult to use and hard to learn). The Product Owners were also asked to rate how useful the features in the software were to them on a scale of one to four (four being essential to them and one being not useful). Last, the users were asked to rate the following statements based on the extent of their agreement with the statement on a scale of one to five (five being that they completely agreed with the statement and one being that they did not agree with the statement at all): ``The application makes my job better'' and ``I feel confident while using the application.''

\paragraph{} The Product Owners were asked about how easy it was to use and learn the primary features of the application that was built for them. [Talk about the importance of having easy to us, learnable software] There are a sum of 16 features represented* in the six applications that data was collected from; these 16 features are composed of the primary features from each application and supporting features that assist the primary feature. The primary features of an application are critical to the application's operation and all to the user's needs. For instance the CAS (Course Administration and Scheduling) application has the following nine features represented: Course Scheduling, Room Preference Selection, Room Resolution, Course Management, User Management, Term Management, Building Management, Course Sequencing. Though all of these features are important as a whole in regards to the functionality of the application, Course Scheduling is the primary feature amongst them because all of the other features are dependent on this feature to be able to  perform their own tasks. Out of the 10 users who responded to the survey, eight users responded that that the features were ``Very easy to use'' and two answered that the features were ``Slight easy to use''. [Refer to Software success paper about why this is good] Furthermore, of the 16 total features across all applications and all the survey respondents, only one feature from one application was marked as ``Difficult to use" by one survey respondent.

The users were asked to rate how useful the primary features of the software were to them and to their daily tasks. This is an important measure because when designing software for a customer, it is important that {insert more words here}. The results of this component of the survey showed that nine out of ten respondents answered that the primary feature within the software that they use is ``Essential''; the tenth respondent marked that the primary feature was ``Useful". Moreover, pertaining to all 16 features in the applications, 15 out of the 16 total features were marked as ``Essential'' and one being marked as ``Useful''. This response shows that all of the applications that were built by the student development team were applicable to the responsibilities attributed to them on a daily basis. [Talk about why having useful software makes it successful].

Last in regards to the user satisfaction measurement of software success, the Product Owners expresses the extent of their agreeance with the following two statements as it pertains to the particular application that they utilize: ``The application makes my job better'' and ``I feel confident while using the application.'' Out of the ten respondents, eight Product Owners said that they ``Strongly Agree'' that the application makes their job better. [Include why having an application improve quality of work is good] One of the two other users answered that they ``Agree'' with the statement and the last remained ``Neutral'' on the question. In response to their confidence about using the application, seven out of ten of the users answered that they ``Strong Agree" with the statement whilst the other three all answered that they ``Agree'' with the statement. [Talk about why confide is good here]

 \subsubsection{Use}
Use of a software weighs the frequency of use, the length of time using the software, and the number of users who actually utilize the software. For this component, the users were given two statements and were asked to rate each statement based on the degree of  their concordance with the statement on a scale of one to five (five being that they completely agreed with the statement and one being that they did not agree with the statement at all.) The statements included the following: ``The application makes my job faster'' and ``My level of competency with software is not relevant when using the application''. The survey also posed questions about how often each user engaged with the application (daily, weekly, monthly, yearly, once per semester, or never)  and how long the user interacts with the application (less than 15 minutes, between 15 and 30 minutes, between 30 minutes and an hour, between one to two hours, and over two hours per session).

\paragraph{} The use of an application is important because '[describe why this is important]'. When asked if the application allows the user to do their job faster, eight of the user responded that they ``Strongly Agree'' with the statement, one user responded that they ``Agree'' with the statement, and one user remained ``Neutral'' on the statement.

The users were asked if their level of software competency was relevant when using the application. In other words, does the user think that they have to have some level technical knowledge in order to use the application. The responses to this question were more varied than previous questions. Two respondents answered that they ``Strongly Agree'' with the statement,  two responded that they ``Agree'' with the statement, five respondents remained ``Neutral'' on the question, and one respondent answered that they ``Disagree'' with the statement.

The users were also asked how often that they interacted with the software while using it. The answers to the qustion regarding the amount of time that the respondents used the software vary because though all of the users are Product Owners of one of the application, they have different roles at the institution and utiliize the software differently. This question was followed by a an optional short answer box in case the respondent wanted to elaborate on their answer about usage. It was found that 50\% of the users engaged with the application monthly, one user engaged the software on a weekly and three users engage the software on a dail basis. One user responded that they engaged with the software once per year, however, this result was expected because this software (Undergraduate Research and Creative Projects Program) is used for students at the institution who participate in summer research, thus the application only needing to be used during the summertime.

Last, the users were asked how long they typically engage with the software whe they use it to do their work. Fifty percent of the users answered that they use the software between 15 to 30 minutes whilst 40\% of the users answered that they engaged with the software from 30 minutes to an hour at a time. Only one user answered that they used the software for fifteen minutes or less.


\subsubsection{Individual Impact}
Individual impact is more of a qualitative measure that is based on the users' personal interactions with the software and its effects that it has had on them. Individual impact includes the user's confidence when using the software, improved personal productivity, the amount of time it takes to complete a task as compared to when the user did not have the software, as well the amount of effort that is put into using the software. For this portion of the survey, the user was asked to estimate how much time they spent completing tasks before the software was implemented and how much time it took to complete the same task when using the software. The user was also asked if the software met their needs that were defined in their initial request for software; this question asked them to declare how much they agreed with the statement on a scale of one to five (five being that they completely agree and one being that they  completely disagreed). Finally, they were asked how much of their day-to-day responsibilities and work depended on their ability to use the newly developed application.

\paragraph{}Retrieving information about how the software impacted each of the users is important because it gives valuable insights to how the application changed the users' typical workflow. Aforementioned in the Use section, though all of the users are Product Owners, they all use the software in different ways. The users were asked to estimate the number of hours they spent completing a specific task before an application was built for them and then to estimate how much time to complete that same task. Two users answered that there was no change in the amount of time they spent accomplishing the same task before and after the application was available. One user abstain from answering the question, noting that their role as an administrator is to provide support to the larger pool of users at the institution and therefore their interactions with that particular were very limited.

\subsubsection{Organizational Impact}
Organizational impact evaluates the organizational goals of the institution, cost reduction, increase in the efficiency and the effectiveness of services. The improvement of the overall institution* directly correlates to the improvements in the users' day-to-day tasks. To gauge what impact the software has had on the institution as a whole, the users were asked to rate the impact, in regards to improvement, the solution that the software provided on a scale of one to four (four being significant improvement, one being no improvement at all). The users were also asked what it would cost in terms of money, stress, errors, and time if the new software was suddenly completely taken down and they had to revert back to their previous processes. This question also required the users to evaluate this on a scale of one to five (five being that it would cost them very much, one not costing them anything at all.) Last, the users were asked to estimate the value of the software in comparison to other applications used at the institution that were not built by the SSDT. They were also to evaluate this comparison on a scale of one to five (five being more valuable than the other software, one being less valuable than the other software).

\paragraph{}
%I think we should include all possible responses for questions like "how important is it" to show what responses the clients were given to choose from
%Yo I now think the idea above is trash. i trust you to translate data into these nice pretty academic terms. -Kat
%Comparison of system to other non SSDT software(moodle,banner, etc)
